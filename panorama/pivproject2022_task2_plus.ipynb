{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ypREH_HYCQSV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import zipfile as zf\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import scipy.ndimage\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import sys \n",
    "from PIL import Image\n",
    "\n",
    "# global variables:\n",
    "MIN_MATCH_COUNT = 24 # minimum number of matches required \n",
    "MAX_ITER = 5000 # number of iterations in ransac\n",
    "NUM_MATCH = 100 # total number of matches that we want to subsample from different locations of the image.\n",
    "                # For every region in the image analysed, if it contains at least a descriptor, and \n",
    "                # if the number of descriptors subsampled is fixed to N = 1, there will be 500 matches. \n",
    "                # Nevertheless, some regions of the image can have no descriptors, so the total number of matches will be reduced. \n",
    "                # For solving this, we can sample N > 1 descriptors from the regions of the image analysed\n",
    "\n",
    "threshold_error = 4 # threshold error for points to be considered inliers, when performing ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(des_src, des_dest, threshold):\n",
    "    '''\n",
    "    Implements the Nearest Neighbor Distance Ratio Test (NNDR) - Equation 4.18 in Section 4.1.3 of \n",
    "    Szeliski - to assign matches between interest points in two images. It also searches for mutual \n",
    "    matches and applies the NNDR test\n",
    "  \n",
    "    A match is between a feature in im1_features and a feature in im2_features. We can\n",
    "    represent this match as a the index of the feature in im1_features and the index\n",
    "    of the feature in im2_features\n",
    "    \n",
    "    :params:\n",
    "    :des_src: an np array of features for interest points in source image\n",
    "    :des_dest: an np array of features for interest points in destination image\n",
    "    \n",
    "    :returns:\n",
    "    :matches: an np array of dimension k x 2 where k is the number of matches. The first\n",
    "            column is an index into im1_features and the second column is an index into im2_features\n",
    "    '''\n",
    "    \n",
    "    global MIN_MATCH_COUNT\n",
    "\n",
    "    matches = []\n",
    "    \n",
    "    # Re-normalize\n",
    "    des_dest_normalize = des_dest / np.linalg.norm(des_dest, axis = 0)\n",
    "    \n",
    "    des_src_normalize = des_src / np.linalg.norm(des_src, axis = 0)\n",
    "    \n",
    "    # cosine similarity (descriptors are L2 normalized) \n",
    "    matrix_similarity = des_src_normalize.T @ des_dest_normalize\n",
    "    \n",
    "    ind_col_matches = np.argmax(matrix_similarity, axis = 1)\n",
    "        \n",
    "    matches = np.concatenate((np.arange(0, des_src.shape[1]).reshape(-1,1), ind_col_matches.reshape(-1, 1)), axis = 1)\n",
    "    final_matches = matches\n",
    "    \n",
    "    # FIND GOOD MATCHES:\n",
    "    # Retrieve top 2 nearest neighbors 1->2.\n",
    "    index_sorted = np.argsort(-matrix_similarity, axis = 1)[:, 0:2]\n",
    "\n",
    "    matrix_distances = np.sqrt(2 - 2 * matrix_similarity)\n",
    "    \n",
    "    mask_good_matches = matrix_distances[list(range(0,matrix_distances.shape[0])), index_sorted[:, 0]] / matrix_distances[list(range(0,matrix_distances.shape[0])), index_sorted[:, 1]] < threshold\n",
    "    \n",
    "    if np.any(mask_good_matches):\n",
    "        good_matches = matches[mask_good_matches, :]\n",
    "        \n",
    "        print(\"good matches/matches - %d/%d\" % (good_matches.shape[0],matches.shape[0]))\n",
    "        \n",
    "        if good_matches.shape[0] > MIN_MATCH_COUNT:\n",
    "            final_matches = good_matches\n",
    "    \n",
    "    # FIND MUTUAL AND GOOD MATCHES: \n",
    "    # Retrieve top 2 nearest neighbors 1->2.\n",
    "    matches_12_top2 = np.argsort(-matrix_similarity, axis = 1)[:, 0:2]\n",
    "    matches_12 = matches_12_top2[:, 0] # Save first NN and match similarity.\n",
    "    \n",
    "    matrix_distances = np.sqrt(2 - 2 * matrix_similarity)\n",
    "    \n",
    "    # Compute Lowe's ratio.\n",
    "    mask1_good_matches = matrix_distances[list(range(0,matrix_distances.shape[0])), matches_12_top2[:, 0]] / matrix_distances[list(range(0,matrix_distances.shape[0])), matches_12_top2[:, 1]] < threshold\n",
    "\n",
    "    # Retrieve top 2 nearest neighbors 1->2.\n",
    "    matches_21_top2 = np.argsort(-matrix_similarity.T, axis = 1)[:, 0:2]\n",
    "    matches_21 = matches_21_top2[:, 0] # Save first NN and match similarity.\n",
    "    \n",
    "    matrix_distances_T = np.sqrt(2 - 2 * matrix_similarity.T)\n",
    "    \n",
    "    # Compute Lowe's ratio.\n",
    "    mask2_good_matches = matrix_distances_T[list(range(0,matrix_distances_T.shape[0])), matches_21_top2[:, 0]] / matrix_distances_T[list(range(0,matrix_distances_T.shape[0])), matches_21_top2[:, 1]] < threshold\n",
    "        \n",
    "    final_mask_good_matches = mask1_good_matches & mask2_good_matches[matches_12]\n",
    "    \n",
    "    # Mutual NN + symmetric ratio test.\n",
    "    ids1 = np.arange(0, matrix_similarity.shape[0])\n",
    "    \n",
    "    mask_mutual_matches = (ids1 == matches_21[matches_12]) & final_mask_good_matches\n",
    "    \n",
    "    if np.any(mask_mutual_matches):\n",
    "        mutual_matches = matches[mask_mutual_matches, :]\n",
    "        \n",
    "        if mutual_matches.shape[0] > MIN_MATCH_COUNT:\n",
    "            final_matches = mutual_matches\n",
    "        \n",
    "        print(\"mutual and good matches/matches - %d/%d\" % (mutual_matches.shape[0],matches.shape[0]))\n",
    "\n",
    "    return final_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siftMatch(img1, img2, sift_path_ref, sift_path_image, threshold = 0.75, N = 1):\n",
    "    \n",
    "    global extract_sift, NUM_MATCH, subsampling\n",
    "    \n",
    "    if extract_sift:\n",
    "        sift = cv2.SIFT_create()\n",
    "        kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "        \n",
    "        m =match_features(des1.T,des2.T, threshold)\n",
    "        src_pts = np.float32([kp1[i].pt for i in m[:,0]]).reshape(-1,2)\n",
    "        dst_pts = np.float32([kp2[i].pt for i in m[:,1]]).reshape(-1,2)\n",
    "    \n",
    "    else:\n",
    "        data_ref = scipy.io.loadmat(sift_path_ref)\n",
    "        dst = data_ref['p'] # (2,N) numpy array, where N is the total number of keypoints\n",
    "        des_dest = data_ref['d'] # (128,N) numpy array, where N is the total number of keypoints\n",
    "        \n",
    "        data_image = scipy.io.loadmat(sift_path_image)\n",
    "        src = data_image['p'] # (2,N) numpy array, where N is the total number of keypoints\n",
    "        des_src = data_image['d'] # (128,N) numpy array, where N is the total number of keypoints\n",
    "        \n",
    "        if subsampling:\n",
    "            h, w, _ = img1.shape\n",
    "            \n",
    "            h_subsampling = math.floor(h/4) \n",
    "            \n",
    "            w_subsampling = math.floor(w * h/(NUM_MATCH*h_subsampling)) \n",
    "            \n",
    "            regions_h = range(0, h+1, h_subsampling)\n",
    "            regions_w = range(0, w+1, w_subsampling)\n",
    "            \n",
    "            des_src_subsampling = np.array([], dtype=np.int64).reshape(des_src.shape[0],0)\n",
    "            src_subsampling = np.array([], dtype=np.int64).reshape(src.shape[0],0)\n",
    "            \n",
    "            id_descriptor = np.arange(des_src.shape[1])\n",
    "            \n",
    "            for i in range(len(regions_h)-1):\n",
    "                h_region_min = regions_h[i]\n",
    "                h_region_max = regions_h[i+1]-1\n",
    "                \n",
    "                for j in range(len(regions_w)-1):\n",
    "                    w_region_min = regions_w[j]\n",
    "                    w_region_max = regions_w[j+1]-1\n",
    "                    \n",
    "                    ind_keypoints_region = (src[0,:] > w_region_min) & (src[0,:] < w_region_max) & (src[1,:] > h_region_min) & (src[1,:] < h_region_max)\n",
    "                    \n",
    "                    if np.any(ind_keypoints_region):\n",
    "                        if len(ind_keypoints_region[ind_keypoints_region == True]) < N:\n",
    "                            num_sampling = len(ind_keypoints_region[ind_keypoints_region == True])\n",
    "                            \n",
    "                        else:\n",
    "                            num_sampling = N \n",
    "                        \n",
    "                        ind_d_des = random.sample(list(id_descriptor[ind_keypoints_region]), num_sampling)\n",
    "                        \n",
    "                        des_src_subsampling = np.concatenate((des_src_subsampling, des_src[:, ind_d_des]), axis = 1)\n",
    "                        src_subsampling = np.concatenate((src_subsampling, src[:, ind_d_des]), axis = 1)\n",
    "            \n",
    "            des_src = des_src_subsampling\n",
    "            src = src_subsampling\n",
    "            \n",
    "        m =match_features(des_src,des_dest, threshold)\n",
    "        \n",
    "        matches_coords = np.concatenate((src[:, m[:,0]], dst[:, m[:, 1]]))\n",
    "        \n",
    "        src_pts = matches_coords[0:2, :].T\n",
    "        dst_pts = matches_coords[2:4, :].T\n",
    "            \n",
    "    return src_pts, dst_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SqLigJ92IisO"
   },
   "outputs": [],
   "source": [
    "def FitHomography(selected_matches, N = 4):\n",
    "    \"\"\" Compute the fitted homography matrix by using N match pairs\n",
    "  \n",
    "   [u]     [X]\n",
    "   [v] = H [Y], \n",
    "   [1]     [1]\n",
    "   being H a 3x3 matrix \n",
    "  \n",
    "   This can be arranged in a system Ax = 0, where x is a column vector with \n",
    "   the parameters of the homography, and A is given by:\n",
    "   A = [X Y 1 0 0 0 -u.X -u.Y -u]\n",
    "       [0 0 0 X Y 1 -v.X -v.Y -v]\n",
    "\n",
    "   For N matches, the above matrix is vertically stacked, with 2 rows per match \n",
    "  \"\"\"\n",
    "    \n",
    "    X = selected_matches[:,0]\n",
    "    Y = selected_matches[:,1]\n",
    "    u = selected_matches[:,2]\n",
    "    v = selected_matches[:,3]\n",
    "    \n",
    "    A = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        row_1 = np.array([X[i], Y[i], 1, 0, 0, 0, -X[i]*u[i], -Y[i]*u[i], -u[i]])\n",
    "        row_2 = np.array([0, 0, 0, X[i], Y[i], 1, -X[i]*v[i], -Y[i]*v[i], -v[i]])\n",
    "        \n",
    "        A.append(row_1)\n",
    "        A.append(row_2)\n",
    "        \n",
    "    A = np.array(A)\n",
    "    \n",
    "    # V = eigvec(A.T @ A), being V.T obtained through Singular Value Decomposition (SVD)\n",
    "    _, _, vT = np.linalg.svd(A)\n",
    "\n",
    "  # vT is a 9×9 matrix\n",
    "  # the solution x is the eigenvector corresponding to the smallest eigenvalue, \n",
    "  # that is, the eigenvector corresponding to the minimum singular value, \n",
    "  # leading to a row vector of 9 columns. Thus, to obtain the calibrated \n",
    "  # homography H, the final solution is to reshape the obtained vector into a \n",
    "  # 3x3 matrix \n",
    "    H = np.reshape(vT[-1,:], (3,3))\n",
    "    \n",
    "    # normalized homography, dividing by the element at (3,3)\n",
    "    H = H/H[2,2]\n",
    "    \n",
    "    return H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TGCDEun9IjWz"
   },
   "outputs": [],
   "source": [
    "def get_errors(all_matches, H):\n",
    "    \"\"\"Compute error or distance between original points and transformed by H. \n",
    "   Return an array of errors for all points\"\"\"\n",
    "    \n",
    "    num_matches = len(all_matches)\n",
    "    \n",
    "    X = all_matches[:,0].reshape(-1, 1)\n",
    "    Y = all_matches[:,1].reshape(-1, 1)\n",
    "    u = all_matches[:,2].reshape(-1, 1)\n",
    "    v = all_matches[:,3].reshape(-1, 1)\n",
    "    \n",
    "    # all matching points in source image\n",
    "    all_p1 = np.concatenate((X, Y, np.ones((len(all_matches),1))), axis = 1)\n",
    "    \n",
    "    # all matching points in template image\n",
    "    all_p2 = np.concatenate((u, v), axis = 1)\n",
    "    \n",
    "    # Transform every point in p1 to estimate p2\n",
    "    estimate_p2homogeneous = H @ all_p1.T\n",
    "    \n",
    "    estimate_p2euclidean = (estimate_p2homogeneous/(estimate_p2homogeneous[-1]))[0:2]\n",
    "    \n",
    "    # Compute error of each matching pair\n",
    "    errors = np.linalg.norm(all_p2 - estimate_p2euclidean.T, axis = 1) \n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T2oXuUgLIp52"
   },
   "outputs": [],
   "source": [
    "def GetHomographyRANSAC(match_coords):\n",
    "    \n",
    "    \"\"\"Function that computes linear (2D) Homography Calibration, implementing RANSAC\n",
    "  for eliminating outliers and align correspondent matches. The main output concerns \n",
    "  a single transformation H that gets the most inliers in the course of all the \n",
    "  iterations. \n",
    "  \n",
    "    Args:\n",
    "        match_coords(numpy.ndarray): In dims (#matched pixels, 4).\n",
    "\n",
    "    Returns:\n",
    "        H(numpy.ndarray): Homography matrix, dims (3, 3).\n",
    "    \"\"\"\n",
    "    \n",
    "    global MAX_ITER, threshold_error\n",
    "    \n",
    "    N = 4 # four matches to initialize the homography in each iteration\n",
    "    \n",
    "    max_inliers = 0 \n",
    "    \n",
    "    # RANSAC procedure    \n",
    "    for itr in range(MAX_ITER): \n",
    "        # Randomly select 4 matched pairs\n",
    "        idx_rand_inliers = random.sample(range(match_coords.shape[0]), N)\n",
    "        \n",
    "        selected_matches = match_coords[idx_rand_inliers, :]\n",
    "        \n",
    "        # compute the homography H by DLT from the N = 4 matched pairs \n",
    "        H = FitHomography(selected_matches)\n",
    "        \n",
    "        # Find inliners \n",
    "        errors = get_errors(match_coords, H)\n",
    "        \n",
    "        idx_inliers = np.where(errors < threshold_error)[0]\n",
    "        \n",
    "        num_inliers = len(idx_inliers) \n",
    "        \n",
    "        # Analise current solution, and if it contains the maximum number of inliers\n",
    "        # amongst all homographies until now fitted, save the current inliers for \n",
    "        # further refinement of the homography in the last step \n",
    "        \n",
    "        if num_inliers > max_inliers:\n",
    "            max_inliers = num_inliers\n",
    "            best_inliers = match_coords[idx_inliers]\n",
    "    \n",
    "    # compute the homography H by DLT from best_inliers  \n",
    "    H = FitHomography(best_inliers, max_inliers)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xlq83FXHpThV"
   },
   "outputs": [],
   "source": [
    "def compute_H_wrt_reference(H_all, ref_image):\n",
    "    \"\"\"\n",
    "  Function that computes new homographies H_map that map every other image *directly* to\n",
    "  the reference image by composing H matrices in H_all. \n",
    "  The homography in H_map that is associated with the reference image\n",
    "  should be the identity matrix, created using eye(3). The homographies in\n",
    "  H_map for the other images (both before and after the reference image)\n",
    "  are computed by using already defined matrices in H_map and H_all. \n",
    "\n",
    "  Args: \n",
    "      H_all(cell array) \n",
    "\n",
    "      ref_image(int): index of the reference image (the first image has index 1)\n",
    "\n",
    "\n",
    "  Returns:\n",
    "      H_map(cell array): 3x3 homographies matrices that map each image into the reference image's\n",
    "  coordinate system.\n",
    "\n",
    "  \"\"\"\n",
    "    num_imgs = len(H_all)+1\n",
    "    \n",
    "    H_map = {}\n",
    "    \n",
    "    key = \"H{}{}\".format(ref_image-1, ref_image-1) \n",
    "    H_map[key] = np.eye(3)\n",
    "    \n",
    "    for i in range(0, ref_image-1): \n",
    "        key = \"H{}{}\".format(i, ref_image-1)  \n",
    "        H_aux = np.eye(3)\n",
    "        \n",
    "        j = i\n",
    "        \n",
    "        while j < ref_image - 1:\n",
    "            key_t = \"H{}{}\".format(j, j+1)\n",
    "            H_aux = H_all[key_t] @ H_aux\n",
    "            j += 1 \n",
    "        \n",
    "        H_map[key] = H_aux\n",
    "    \n",
    "    for i in range(ref_image, num_imgs):\n",
    "        key = \"H{}{}\".format(i, ref_image-1)  # H10\n",
    "        H_aux = np.eye(3)\n",
    "        \n",
    "        j = i -1 \n",
    "        \n",
    "        while j>= ref_image-1:\n",
    "            key_t = \"H{}{}\".format(j, j+1)\n",
    "            H_inv = np.linalg.inv(H_all[key_t])\n",
    "            H_aux = H_inv/H_inv[2,2] @ H_aux\n",
    "            j -= 1\n",
    "        \n",
    "        H_map[key] = H_aux\n",
    "    \n",
    "    return H_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Homography(H):\n",
    "    \"\"\"\n",
    "  Check if homography is reasonable, according to certain criteria:\n",
    "  - If the determinant of the homography det(H) is very close to 0, H is \n",
    "   close to singular;\n",
    "  \n",
    "  - If condition number of H (ratio of the first-to-last singular value) is\n",
    "    infinite, the matrix H is singular, and if it is too large, H is \n",
    "    ill-conditioned. In non-mathematical terms, an ill-conditioned problem \n",
    "    is one where, for a small change in the inputs, there is a large \n",
    "    change in the output, that is, H is very sensitive to changes or errors \n",
    "    in the input. This means that the correct solution/answer to the \n",
    "    equation becomes hard to find;\n",
    "        \n",
    "  - If det(H) < 0, the homography is not conserving the orientation, \n",
    "    being orientation-reversing. This is not suitable, except if we are \n",
    "    watching the object in a mirror. Nevertheless, sift/surf descriptors \n",
    "    are not done to be mirror invariant, so if it was the case we would \n",
    "    probably not have good maches. \n",
    "        \n",
    "  An exactly singular matrix means that it is not invertible. If the above \n",
    "  criteria is verified, more pratically the matrix H is non-invertible. \n",
    "  In the context of homographies, it means that points in one 2D image are mapped\n",
    "  to a less-than-2D subspace in the other image (a line, a point). A \n",
    "  nearly singular matrix is indicative of a rather extreme warp.  \n",
    "  \"\"\"\n",
    "    \n",
    "    #Conditions to accertain that the resultant homography H is free of \n",
    "    #singularities. If one of the condition is satisfied, the Homography H from \n",
    "    #image space to reference image space is not reasonable, according \n",
    "    #to the defined criteria \n",
    "    if  np.linalg.det(H) < 1 and np.linalg.cond(H[0:2, 0:2]) > 2:\n",
    "        \n",
    "        # In the condition number, only the top-left 2x2 matrix is considered, \n",
    "        # thus omitting the z-dependence of the transformation, which should be \n",
    "        # irrelevant because we know that z will always be fixed to 1 on the input\n",
    "        \n",
    "        H = np.zeros((3,3))\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3V1sRAB67IYr"
   },
   "outputs": [],
   "source": [
    "def get_blank_canvas(H_warp, ind_image_warp, image_paths, ref_img):\n",
    "    \"\"\"\n",
    "  Function that computes the size of the panorama using forward warping. Before warping \n",
    "  each of the images, the size of the output panorama image is computed and \n",
    "  initialized from the range of warped image coordinates for each input image.\n",
    "  \"\"\"\n",
    "    \n",
    "    num_imgs = len(H_warp)\n",
    "    \n",
    "    # Initialize the limits of the output panorama image\n",
    "    min_crd_canvas = np.array([np.inf, np.inf])\n",
    "    max_crd_canvas = np.array([-np.inf, -np.inf])\n",
    "    \n",
    "    limits_all = []\n",
    "    \n",
    "    # mapping the coordinates of the four corners from each source image using forward warping to determine its coordinates in \n",
    "    # the output image. \n",
    "    \n",
    "    for i in range(num_imgs):\n",
    "        ind_image = ind_image_warp[i]\n",
    "        \n",
    "        image = img.imread(image_paths[ind_image])\n",
    "        img_h, img_w, _ = image.shape\n",
    "        \n",
    "        key = \"H{}{}\".format(ind_image, ref_img-1)\n",
    "        H = H_warp[key]\n",
    "        \n",
    "        # create a matrix with the coordinates (homogeneous) of the four corners \n",
    "        # of the current image\n",
    "        corners_img = np.array([[0, 0, 1], [0, img_h,1], [img_w, img_h,1], [img_w, 0,1]])\n",
    "            \n",
    "        # Map each of the 4 corner's coordinates into the coordinate system of\n",
    "        # the reference image\n",
    "        canvas_crd_corners = H @ corners_img.T\n",
    "        canvas_crd_corners = (canvas_crd_corners / canvas_crd_corners[-1, :])[0:2, :]\n",
    "            \n",
    "        limits_all.append(canvas_crd_corners.T)\n",
    "            \n",
    "        # Limits of the current warped image \n",
    "        min_crd_canvas_cur = np.amin(canvas_crd_corners.T, axis=0) # min_x, min_y\n",
    "        max_crd_canvas_cur = np.amax(canvas_crd_corners.T, axis=0) # max_x, max_y\n",
    "            \n",
    "        # Update the limits of the output image \n",
    "        min_crd_canvas = np.floor(np.minimum(min_crd_canvas_cur, min_crd_canvas)) # min_x, min_y\n",
    "        max_crd_canvas = np.ceil(np.maximum(max_crd_canvas_cur, max_crd_canvas)) # max_x, max_y\n",
    "        \n",
    "    # Compute output image size \n",
    "    min_x = min_crd_canvas[0]\n",
    "    max_x = max_crd_canvas[0]\n",
    "    min_y = min_crd_canvas[1]\n",
    "    max_y = max_crd_canvas[1]\n",
    "    \n",
    "    width_canvas = max_x - min_x + 1\n",
    "    height_canvas = max_y - min_y + 1\n",
    "    \n",
    "    # output image array initialized to all black pixels\n",
    "    canvas_img = np.zeros((int(height_canvas), int(width_canvas), 3), dtype=np.int64)\n",
    "    \n",
    "    # Compute offset of the upper-left corner of the reference image relative\n",
    "    # to the upper-left corner of the output image\n",
    "    offset = min_crd_canvas.astype(np.int64) # [x_offset, y_offset]\n",
    "    \n",
    "    # Find limits of panorama\n",
    "    lims = np.concatenate(limits_all,axis=0)\n",
    "    \n",
    "    for i in range(int(lims.shape[0]/4)):\n",
    "        lims_i = np.concatenate((lims[4*i:4 + 4*i], lims[None, 4*i, :]), axis = 0)\n",
    "        plt.plot(lims_i[:, 0], -lims_i[:,1])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return canvas_img, offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aNfKmjB99mzG"
   },
   "outputs": [],
   "source": [
    "def image_warping(panorama_height, panorama_width, offset, H, img):\n",
    "    \"\"\"\n",
    "  Function that warps every input image on to the panorama, using inverse warping\n",
    "  to map each pixel in the output image into the planes defined by the source images. \n",
    "  If forward warping was used to map every pixel from each source image, there will \n",
    "  be holes (i.e., some pixels in the output image will not be assigned an RGB \n",
    "  value from any source image, and remain black) in the final output image.\n",
    "  \"\"\"\n",
    "    \n",
    "    x_offset = -offset[0]\n",
    "    y_offset = -offset[1]\n",
    "    \n",
    "    # Create a list of all pixels' coordinates in output image\n",
    "    x,y = np.meshgrid(range(panorama_width), range(panorama_height))\n",
    "    \n",
    "    # Create homogeneous coordinates for each pixel in output image, considering \n",
    "    # the translation offset vector \n",
    "    x_coords = x.flatten().reshape(1,-1) - x_offset\n",
    "    y_coords = y.flatten().reshape(1,-1) - y_offset\n",
    "    \n",
    "    grid_coords = np.concatenate((x_coords, y_coords, np.ones((1, x_coords.shape[1]))))\n",
    "    \n",
    "    # Perform inverse warp to compute coordinates in current input image\n",
    "    image_coords = np.linalg.solve(H, grid_coords)\n",
    "    \n",
    "    # To get the warped coordinates, we must divide the first and second coordinates by \n",
    "    # z to obtain the new x and y (euclidean coordinates)\n",
    "    z = image_coords[None, 2, :]  \n",
    "    warp_coords = image_coords[0:2, :]/np.concatenate((z,z))\n",
    "    \n",
    "    # Reshape the pixel grid to have the same size as the panorama \n",
    "    x_warp = np.reshape(warp_coords[None, 0, :], (panorama_height, panorama_width))\n",
    "    y_warp = np.reshape(warp_coords[None, 1, :], (panorama_height, panorama_width))\n",
    "    # Note:  Some values will return as NaN (\"not a number\") because they\n",
    "    # map to points outside the domain of the input image\n",
    "    \n",
    "    # Warped Image array that will contain RGB color maps obtained through inverse \n",
    "    # mapping \n",
    "    I_WarpColorMaps = np.zeros((panorama_height, panorama_width, 3))\n",
    "    \n",
    "    # Color interpolation, by sampling a color value for each pixel in source image. \n",
    "    # By doing this we won't have any black pixels or gaps in the warpped image, \n",
    "    # (a kind of undersampling artifact)\n",
    "    \n",
    "    for channel in range(3):\n",
    "        # When mapping pixel locations,  some pixels in the output warped image will not \n",
    "        # map to a pixel in a given source image because the output pixel’s coordinates \n",
    "        # map outside the domain of the source image.  For solving this, we use bilinear \n",
    "        # interpolation (order = 1) for assigning the color value in the warped image\n",
    "        \n",
    "        I_WarpColorMaps[:, :, channel] = scipy.ndimage.map_coordinates(img[:, :, channel].astype(float), [y_warp, x_warp], order = 1)\n",
    "    \n",
    "    # color pixel warping, by converting I_WarpColorMaps into an unsigned 8-bit integer, \n",
    "    # with the elements of an uint8 ranging from 0 to 255\n",
    "    warped_image = I_WarpColorMaps.astype('uint8')\n",
    "\n",
    "    return warped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6cmOQP6M23Mg"
   },
   "outputs": [],
   "source": [
    "def alpha_channel(img, epsilon=0.001):\n",
    "    \"\"\"\n",
    "  Funtion that computes the alpha channel of an RGB image.\n",
    "\n",
    "  Args:\n",
    "     img is an RGB image. \n",
    "\n",
    "     epsilon (float): value to guarantee that the alpha channel has non-zero\n",
    "     values, otherwise a division-by-zero error will be encounter \n",
    "     when performing blending \n",
    "  \n",
    "  Returns:\n",
    "     im_alpha has the same size as im_input. Its intensity is between\n",
    "     epsilon and 1, inclusive.\n",
    "  \"\"\"\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # binary image that has 1s within the warped image and 0s beyond the edges of \n",
    "    # the warped image \n",
    "    im_bw = cv2.threshold(img_gray, 0.5, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # alpha channel where the value of alpha for the input image is 1 at its \n",
    "    # center pixel and decreases linearly to epsilon  at all the border pixels\n",
    "    im_alpha = scipy.ndimage.distance_transform_edt(im_bw)\n",
    "    \n",
    "    # normalize the distances to be in the interval [epsilon, 1].\n",
    "    im_alpha = (im_alpha+epsilon)/np.max(im_alpha) \n",
    "    \n",
    "    return im_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RAMrHeFK2IUr"
   },
   "outputs": [],
   "source": [
    "def blending(img1, img2):\n",
    "    \"\"\"\n",
    "  Function that blends two warped images together, overlapping pixel color values. \n",
    "  The simplest way to create a final composite is by averaging the pixel values \n",
    "  where the two images overlap, or by using the pixel values from one of the \n",
    "  images.\n",
    "  Simple averaging usually does not work very well, since exposure differences, ´\n",
    "  misregistrations, and scene movement are all very visible. \n",
    "  A better approach to averaging is to weight pixels near the center of the \n",
    "  image more heavily and to down-weight pixels near the edges, being this \n",
    "  enconded into a alpha channel. \n",
    "  This is called feathering (Section 9.3.2 in the Szeliski book)\n",
    "\n",
    "  Each pixel (x, y) in image Ii is represented as \n",
    "  Ii(x, y) = (αi*R, αi*G, αi*B, αi) where (R,G,B) are the color values at the \n",
    "  pixel and αi its alpha channel \n",
    "  \n",
    "  Pixel value of (x, y) in the stitched output image is computed has:\n",
    "  [(α1*R, α1*G, α1*B) + (α2*R, α2*G,α2*B) ] / (α1+α2).\n",
    "\n",
    "  Args:\n",
    "  img1 and img2 are both RGB images of the same size, having \n",
    "  been warped to the same coordinate frame \n",
    "  \n",
    "  Output:\n",
    "  im_blended has the same size and data type as the input images\n",
    "\n",
    "  \"\"\"\n",
    "    \n",
    "    if feathering:\n",
    "        # alpha channel that contains weights for blending the images \n",
    "        alpha1 = alpha_channel(img1)\n",
    "        alpha2 = alpha_channel(img2)\n",
    "        \n",
    "        im_blended = np.zeros(img1.shape)\n",
    "        \n",
    "        red_blending = (alpha1 * img1[:, :, 0] + alpha2 * img2[:,:,0])/(alpha1 + alpha2)\n",
    "        green_blending = (alpha1 * img1[:, :, 1] + alpha2 * img2[:,:,1])/(alpha1 + alpha2)\n",
    "        blue_blending = (alpha1 * img1[:, :, 2] + alpha2 * img2[:,:,2])/(alpha1 + alpha2)\n",
    "        \n",
    "        im_blended[:,:,0] = red_blending \n",
    "        im_blended[:,:,1] = green_blending \n",
    "        im_blended[:,:,2] = blue_blending\n",
    "        \n",
    "        # convert into an unsigned 8-bit integer, for the values of each channel to\n",
    "        # range from 0 to 255\n",
    "        im_blended = im_blended.astype('uint8')\n",
    "    \n",
    "    else:\n",
    "        # average blending \n",
    "        \n",
    "        mask_a = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "        mask_b = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        a_and_b = cv2.bitwise_and(mask_a, mask_b)\n",
    "        overlap_area_mask = cv2.threshold(a_and_b, 1, 255, cv2.THRESH_BINARY)[1]\n",
    "        \n",
    "        overlap_pixels = (cv2.bitwise_and(img1, img1, mask = overlap_area_mask.astype('uint8')) + cv2.bitwise_and(img2, img2, mask = overlap_area_mask.astype('uint8')))/2\n",
    "        \n",
    "        im_blended = cv2.bitwise_and(img1, img1, mask = np.logical_not(overlap_area_mask).astype('uint8')) + cv2.bitwise_and(img2, img2, mask = np.logical_not(overlap_area_mask).astype('uint8')) + overlap_pixels\n",
    "                \n",
    "        im_blended = im_blended.astype('uint8')\n",
    "                \n",
    "    return im_blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7Yp71lRiKgAu"
   },
   "outputs": [],
   "source": [
    "def pivproject2022_task2_plus(ref_image, path_to_input_folder, path_to_output_folder, extract_sift, subsampling, cv2WarpPerspective, feathering):\n",
    "    \"\"\"\n",
    "  Compute the homographies between images in a directory and a reference image\n",
    "\n",
    "    ref_image: integer with index number of the frame that will be the reference \n",
    "    image. First image is index=1. Ideally, the one in the middle of the sequence \n",
    "    of input images so that there is less distortion resulting mosaic.\n",
    "    \n",
    "    path_to_input_folder: string with the path to the input folder, where input images \n",
    "                           and keypoints are stored. Images are named rgb_number.jpg \n",
    "                           (or rgb_number.png) and corresponding keypoints are named \n",
    "                           rgbsift_number.mat\n",
    "\n",
    "    \n",
    "    path_to_output_folder: string with the path where homographies with respect to the reference\n",
    "    image are stored\n",
    "\n",
    "  \"\"\"\n",
    "    \n",
    "    # Check if path_to_input_folder was passed. If not, \"No_path\" is assigned\n",
    "    if not('path_to_input_folder' in locals()):\n",
    "        path_to_input_folder = \"No_path\";\n",
    "        \n",
    "    # Check if output directory exists. If not, output directory is created \n",
    "    if not(os.path.isdir(path_to_output_folder)):\n",
    "        os.mkdir(path_to_output_folder)\n",
    "        \n",
    "    # Get input rgb images\n",
    "    rgb_paths = []\n",
    "    sift_paths = []\n",
    "        \n",
    "    for im_path in glob.glob(path_to_input_folder+'/*.jpg'):\n",
    "        rgb_paths.append(im_path)\n",
    "    \n",
    "    if len(rgb_paths) == 0:\n",
    "        print('ERROR: In the specified path there aren\\'t image input files')\n",
    "        return \n",
    "    \n",
    "    else: \n",
    "        #Ordering the rgb_paths array, in such a way that consecutive frames follow \n",
    "        #each other  \n",
    "        image_paths = sorted(rgb_paths) \n",
    "    \n",
    "    if not(extract_sift):\n",
    "        print('Searching for sift .mat files')\n",
    "        \n",
    "        for im_path in glob.glob(path_to_input_folder+'/*.mat'):\n",
    "            sift_paths.append(im_path)\n",
    "            \n",
    "            if len(sift_paths) != 0:\n",
    "                \n",
    "                sift_paths_ordered = sorted(sift_paths)\n",
    "            \n",
    "            else:\n",
    "                extract_sift = True\n",
    "                print('In the specified path there aren\\'t sift input files. Thus, a sift function will be used to extract matching points')\n",
    "\n",
    "    # Get Reference image\n",
    "    try: \n",
    "        reference_image = img.imread(image_paths[ref_image - 1])\n",
    "        \n",
    "    except:\n",
    "        print('ERROR: The index for the reference image is out of bounds. Please select an index between 1 and %d' % (len(rgb_paths)))\n",
    "        return\n",
    "    \n",
    "    H_all = {}\n",
    "        \n",
    "    # compute homography matrices between adjacent input images. Homography matrices \n",
    "    # between adjacent input images are then stored in a cell array H_all.\n",
    "    \n",
    "    for i in range(len(image_paths)-1):\n",
    "        image_1_path = image_paths[i]\n",
    "        image_2_path = image_paths[i+1]\n",
    "        \n",
    "        print(\"Processing {} & {}\".format(image_1_path, image_2_path))\n",
    "        \n",
    "        image_1 = img.imread(image_1_path)\n",
    "        image_2 = img.imread(image_2_path)\n",
    "        \n",
    "        if extract_sift:\n",
    "            sift_path1 = None\n",
    "            sift_path2 = None\n",
    "        \n",
    "        else:\n",
    "            sift_path1 = sift_paths_ordered[i]\n",
    "            sift_path2 = sift_paths_ordered[i+1]\n",
    "        \n",
    "        key = 'H{}{}'.format(i, i+1)\n",
    "        \n",
    "        #try:\n",
    "            # coordinates of the matches between image and template \n",
    "        m_coords_img, m_coords_temp = siftMatch(image_1, image_2, sift_path2, sift_path1, N = 4)\n",
    "        match_coords = np.append(m_coords_img, m_coords_temp, axis = 1)\n",
    "        #except:\n",
    "         #   print('ERROR: check format of directory, as OpenCV only accepts ASCII characters for image paths')\n",
    "         #   return \n",
    "        \n",
    "        try:\n",
    "            threshold_error = 4\n",
    "            \n",
    "            H_all[key] = GetHomographyRANSAC(match_coords)\n",
    "            \n",
    "        except: \n",
    "            print('ERROR: RANSAC failed to compute homography. Check if there are enough matching keypoints.')\n",
    "            \n",
    "    \n",
    "    # Compute new homographies H_map that map every other image *directly* to\n",
    "    # the reference image \n",
    "    H_map = compute_H_wrt_reference(H_all, ref_image)\n",
    "     \n",
    "        \n",
    "    H_warp = {} # cell array that will contain the reasonable homographies between images and reference image, \n",
    "                # so that only these images are warped \n",
    "        \n",
    "    ind_image_warp = [] # list that will contain the index of the images that will be warped \n",
    "    \n",
    "    for i in range(len(H_map)):\n",
    "        key = \"H{}{}\".format(i, ref_image-1)\n",
    "        H = H_map[key]\n",
    "        \n",
    "        print(\"image \"+str(i))\n",
    "        print(\"np.linalg.det(H):\", np.linalg.det(H)) \n",
    "        print(\"np.linalg.cond(H[0:2, 0:2]):\", np.linalg.cond(H[0:2, 0:2]))\n",
    "            \n",
    "        #H = Check_Homography(H)\n",
    "\n",
    "        if np.array_equal(H, np.zeros((3,3))):\n",
    "            print(\"H{}{} is not reasonable\".format(i, ref_image-1))\n",
    "        else:\n",
    "            print(\"H{}{} is reasonable\".format(i, ref_image-1))\n",
    "            H_warp[key] = H\n",
    "            ind_image_warp.append(i)\n",
    "\n",
    "        # saving homographies with respect to the reference image \n",
    "        file_name = os.path.split(image_paths[i])[1]\n",
    "        H_output_path = path_to_output_folder + '/' + 'H_' + file_name[4:8] + '.mat'\n",
    "        scipy.io.savemat(H_output_path, {'H':H})\n",
    "        \n",
    "    canvas_img, offset = get_blank_canvas(H_warp, ind_image_warp, image_paths, ref_image)\n",
    "    \n",
    "    panorama_height, panorama_width, _ = canvas_img.shape\n",
    "    \n",
    "    # cell array that contains warped input images on the output canvas panorama \n",
    "    warped_images = {}\n",
    "    \n",
    "    for i in range(len(H_warp)):\n",
    "        ind_image = ind_image_warp[i]\n",
    "                       \n",
    "        key = \"H{}{}\".format(ind_image, ref_image-1)\n",
    "        H = H_warp[key]\n",
    "        \n",
    "        image = cv2.imread(image_paths[ind_image])\n",
    "        \n",
    "        if cv2WarpPerspective:\n",
    "            # Combine homography with the translation offset vector \n",
    "            translation_mat = np.array([[1, 0, -offset[0]], [0, 1, -offset[1]], [0, 0, 1]])\n",
    "            H = np.dot(translation_mat, H)\n",
    "            warped_images[i] = cv2.warpPerspective(image, H, (canvas_img.shape[1], canvas_img.shape[0]), flags = cv2.INTER_NEAREST)\n",
    "            \n",
    "        else:\n",
    "            warped_images[i] = image_warping(panorama_height, panorama_width, offset, H, image)\n",
    "    \n",
    "    # Initialize output image to black (0)\n",
    "    panorama_image = np.zeros((panorama_height, panorama_width,3))\n",
    "    \n",
    "    panorama_image = warped_images[0] \n",
    "    \n",
    "    for i in range(1,len(warped_images)):\n",
    "        panorama_image = blending(np.float32(panorama_image), np.float32(warped_images[i]))\n",
    "    \n",
    "    plt.title(\"Panorama\")\n",
    "    plt.imshow(panorama_image)\n",
    "    plt.show()\n",
    "    \n",
    "    image_output_path = path_to_output_folder + '/mosaic_' + str(ref_image) + '.png'\n",
    "    cv2.imwrite(image_output_path, panorama_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = 3\n",
    "path_to_input_folder = 'pittsburgh'\n",
    "path_to_output_folder = 'Output_folder_pittsburgh'\n",
    "\n",
    "extract_sift = 0 # variable that defines if the extraction of sift keypoints and descriptors is to \n",
    "                 # be performed (set to 1, 0 otherwise) \n",
    "\n",
    "subsampling = 1 # variable that defines if subsampling of the source image \n",
    "                # descriptors is to be performed (set to 1, 0 otherwise), when \n",
    "                # performing the matching\n",
    "    \n",
    "cv2WarpPerspective = 1 # variable that defines if image warping is to be performed by \n",
    "                       # cv2WarpPerspective built-in function (set to 1) or a function \n",
    "                       # developed by the group (set to 0) \n",
    "\n",
    "feathering = 0 # variable that defines if feathering is going to be performed (set to 1) or not \n",
    "               # (set to 0) not for blending the warped images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xmx6qS2Y3PHG",
    "outputId": "8a6fcd55-2d16-4a06-9ace-a201d94053f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for sift .mat files\n",
      "Processing pittsburgh\\rgb_0001.jpg & pittsburgh\\rgb_0002.jpg\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-372ebaa7e69d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpivproject2022_task2_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_input_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_output_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_sift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsampling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2WarpPerspective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeathering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-b83f57bcc483>\u001b[0m in \u001b[0;36mpivproject2022_task2_plus\u001b[1;34m(ref_image, path_to_input_folder, path_to_output_folder, extract_sift, subsampling, cv2WarpPerspective, feathering)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mimage_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_1_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mimage_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_2_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mextract_sift\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1491\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[0;32m   1492\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1493\u001b[1;33m                 pil_to_array(image))\n\u001b[0m\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mpil_to_array\u001b[1;34m(pilImage)\u001b[0m\n\u001b[0;32m   1624\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpilImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'RGBA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RGBX'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1625\u001b[0m         \u001b[1;31m# return MxNx4 RGBA, MxNx3 RBA, or MxN luminance array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1626\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpilImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1627\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpilImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I;16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m         \u001b[1;31m# return MxN luminance array of uint16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"encoder error {s} in tobytes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtobitmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pivproject2022_task2_plus(ref_image, path_to_input_folder, path_to_output_folder, extract_sift, subsampling, cv2WarpPerspective, feathering)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
